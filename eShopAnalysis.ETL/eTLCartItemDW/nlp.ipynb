{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "usage_instruction_df = pd.read_csv('resources/product_usage_instruction.csv')\n",
    "usage_instruction_df.dropna(inplace=True, subset=['ProductUsageInstruction'])\n",
    "usage_instruction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.tokens import Token\n",
    "from spacy import displacy\n",
    "# python -m spacy download en_core_web_sm \n",
    "# remember to run the above command in the terminal to download the model: https://spacy.io/usage/spacy-101\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "map_to_lemmatized = lambda token: token.lemma_.lower()\n",
    "filter_pos = lambda token: token.pos_ != \"AUX\" and token.pos_ != \"DET\" and token.pos_ != \"PRON\" and token.pos_ != \"SCONJ\" and token.pos_ != \"PUNCT\" and token.pos_ != \"SYM\" and token.pos_ != \"X\" and token.pos_ != \"SPACE\"\n",
    "filter_noun = lambda token: token.lemma_ != \"product\" and token.lemma_ != \"warning\" and token.text != \"Warning\" and token.text != \"Warnings\"\n",
    "filter_stop_words = lambda token: not token.is_stop\n",
    "\n",
    "def map_list_tokens_to_sublists_tokens(list: list[Token]) -> list[list[Token]]:\n",
    "    sublists = []\n",
    "    sublist = []\n",
    "    for idx, token in enumerate(list):\n",
    "        if idx == len(list) - 1:\n",
    "            sublist.append(token)\n",
    "            sublists.append(sublist)\n",
    "            break\n",
    "        if (token.pos_ == \"CCONJ\" or (token.pos_ == \"PART\" and token.text != \"to\")) and (list[idx+1].pos_ == \"VERB\" or list[idx+1].dep_ == \"xcomp\" or list[idx+1].dep_ == \"acomp\"):\n",
    "            sublists.append(sublist)\n",
    "            sublist = []\n",
    "            if token.text == \"not\":\n",
    "                sublist.append(token)\n",
    "        else:\n",
    "            sublist.append(token)\n",
    "    return sublists\n",
    "\n",
    "def map_to_one_string(list: list[str]) -> str:\n",
    "    return ' '.join([text_token for text_token in list])\n",
    "\n",
    "def filter_simple_structure_sublists(sublists: list[list[Token]]) -> list[list[Token]]:\n",
    "    filtered_sublists = []\n",
    "    \n",
    "    raw_regex__dep_list_to_keep = [\"^neg ROOT.*\", \"^amod ROOT$\", \"^advmod ROOT$\"]\n",
    "    # not (main verb), direct use, better hot\n",
    "    raw_regex__pos_list_to_keep = [\"^VERB.*\", \"^ADV VERB.*\", \"^ADV PART VERB.*\" , \"^PART VERB.*\"] \n",
    "    # directly to use , served best chilled, served best chilled or over ice, not use ...\n",
    "    regex__pos_list_to_keep = list(map(lambda regex:re.compile(regex), raw_regex__pos_list_to_keep))\n",
    "    regex__dep_list_to_keep = list(map(lambda regex:re.compile(regex), raw_regex__dep_list_to_keep))\n",
    "    for sublist in sublists:\n",
    "        grammar_tokens_dep = list(map(lambda token: token.dep_, sublist))\n",
    "        grammar_tokens_pos = list(map(lambda token: token.pos_, sublist))\n",
    "        dep_str = map_to_one_string(grammar_tokens_dep).strip() # \"ROOT advmod\"\n",
    "        pos_str = map_to_one_string(grammar_tokens_pos).strip() # \"VERB ADV\"\n",
    "        # print(\"Regex dep: \", regex__dep_list_to_keep, '\\n')\n",
    "        # print(\"Regex pos: \", regex__pos_list_to_keep, '\\n')\n",
    "        # print(\"Dep str: \", dep_str, '\\n')\n",
    "        # print(\"Pos str: \", pos_str, '\\n')\n",
    "        # \\ multiple lines(do not have space)\n",
    "        if (any(regex.match(dep_str) for regex in regex__dep_list_to_keep)\\\n",
    "            or any(regex.match(pos_str) for regex in regex__pos_list_to_keep))\\\n",
    "            and not re.findall(r'.*VERB.*', pos_str).count(\".*VERB.*\") > 1: #TODO fix later, this exclude multiple verb, except for the one in the list\n",
    "                filtered_sublists.append(sublist)\n",
    "        else:\n",
    "            print(\"COUNT VERB: \", re.findall(r'.*VERB.*', pos_str).count(\"VERB\"), '\\n')\n",
    "            print(\"Not match: \", dep_str, '\\n')\n",
    "            print(\"Not match: \", pos_str, '\\n')\n",
    "            pass\n",
    "    return filtered_sublists\n",
    "\n",
    "\n",
    "# check this link to understand pos tags: \n",
    "# https://spacy.io/api/annotation#pos-tagging\n",
    "# https://stackoverflow.com/a/40288324\n",
    "# filter_auxiliary_verb = lambda token: token.pos_ != \"AUX\" and token.pos_ != \"PART\" and token.pos_ != \"DET\" and token.pos_ != \"PRON\" \n",
    "# and token.pos_ != \"CCONJ\" and token.pos_ != \"SCONJ\" and token.pos_ != \"PUNCT\" and token.pos_ != \"SYM\" and token.pos_ != \"X\" and token.pos_ != \"SPACE\"\n",
    "for _, usage_instruction_df_row in usage_instruction_df.iterrows():\n",
    "    usage_instruction = usage_instruction_df_row['ProductUsageInstruction']\n",
    "    usage_instruction = re.sub(r'[^\\w\\s]', '', usage_instruction)\n",
    "    usage_instruction = re.sub(r'^\\d?', '', usage_instruction)\n",
    "    usage_instruction = usage_instruction.strip()\n",
    "    print(f\"Original usage instruction: {usage_instruction}\", '\\n')\n",
    "\n",
    "    doc = nlp(usage_instruction)\n",
    "    tokens_with_pos = [token for token in doc]\n",
    "    token_filtered_noun = list(filter(filter_noun, tokens_with_pos))\n",
    "    tokens_without_pos = list(filter(filter_pos, token_filtered_noun))\n",
    "    print(\"Token without pos:\", tokens_without_pos, '\\n')\n",
    "    # tokens_lemm = [token.lemma_.lower() for token in tokens_without_pos]\n",
    "    # tokens = list(map(map_chill_to_chilled, tokens_lemm))\n",
    "    # DO NOT LEMMATIZE, WE NEED TO RETAIN THE GRAMMER STRUCTURE\n",
    "\n",
    "    tokens_lemm = [token for token in tokens_without_pos]\n",
    "    tokens = list(map(lambda token: token.text.lower(), tokens_lemm))\n",
    "    print(\"Final token(not lemm): \",tokens, '\\n')\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    token_list = [token for token in doc]\n",
    "    sublists = map_list_tokens_to_sublists_tokens(token_list)\n",
    "    print(\"Sublists: \", sublists, '\\n')\n",
    "\n",
    "    # Remove stop words, ok do not use this since it make the sentence structure lost\n",
    "    # for sublist in sublists:\n",
    "    #     sublist = list(filter(filter_stop_words, sublist))\n",
    "    #     sublist = list(map(map_to_lemmatized, sublist))\n",
    "    #     print(sublist, '\\n')\n",
    "    # print(\"Sublists after removing stop words: \", sublists, '\\n')\n",
    "\n",
    "    # Remove simple structure\n",
    "    sublists = filter_simple_structure_sublists(sublists)\n",
    "    print(\"Sublists after removing simple structure: \", sublists, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "    # for sublist in sublists:\n",
    "    #     grammar_tokens_dep = [token.dep_ for token in sublist]\n",
    "    #     grammar_tokens_pos = [token.pos_ for token in sublist]\n",
    "        # print(\"Grammer token Dep to string: \", map_to_one_string(grammar_tokens_dep), '\\n')\n",
    "        # print(\"Grammer token Pos to string: \", map_to_one_string(grammar_tokens_pos), '\\n')\n",
    "        # print(\"Grammar token Dep: \",grammar_tokens_dep, '\\n')\n",
    "        # print(\"Grammar token Pos: \",grammar_tokens_pos, '\\n')\n",
    "\n",
    "    # Lemmatize, Stemming for final result, then ready to tranform to name of usage instruction type\n",
    "    import nltk\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    def map_token_to_stemmed(token_text: str) -> str:\n",
    "        return porter_stemmer.stem(token_text)\n",
    "    \n",
    "    for sublist in sublists:\n",
    "        # sublist = list(filter(filter_stop_words, sublist))\n",
    "        # print(\"Sublist after removing stop words and before lemmed: \",sublist, '\\n')\n",
    "        # DO NOT REMOVE STOP WORD SINCE IT HAVE MEANING LIKE NOT, TO, ...\n",
    "        print(\"Sublist before lemmed: \", sublist, '\\n')\n",
    "        sublist = list(map(map_to_lemmatized, sublist))\n",
    "        print(\"Sublist before stemmed and after lemmed: \", sublist, '\\n')\n",
    "        sublist = list(map(map_token_to_stemmed, sublist))\n",
    "        print(\"Sublist after stemmed: \",sublist, '\\n')\n",
    "\n",
    "    print(\"=========================================================================================================================\")\n",
    "    # displacy.render(doc, style=\"dep\")\n",
    "    # displacy.render(doc, style=\"ent\")\n",
    "    # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "    # print(token.vector_norm)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
